{"cells":[{"cell_type":"markdown","metadata":{"id":"V7GIDkK7P8A1"},"source":["### Nuißl Sandra, 14.08.2023\n","## \"Empirische Evaluation von ‚State Of The Art‘ Topic Modeling Ansätze am Beispiel von Produktreviews für die Entscheidungsunterstützung in Unternehmen\"\n","### - Beschleunigung der Data Preperation mithilfe von Pyspark -\n","![Step1](https://github.com/Sannui/Masterarbeit_Nuissl_Sandra/blob/main/Titelbild.jpg)\n","<center><img src=\"https://github.com/Sannui/Masterarbeit_Nuissl_Sandra/blob/main/Titelbild.jpg\" height=\"300px\" width=\"1100px\"/></center>\n","<center><font size=\"1\">https://www.cloudways.com/blog/wp-content/uploads/Product-Review-1024x576.jpg</font></center>\n","\n","\n","<hr>\n","\n","## **Inhaltsverzeichnis**\n","\n","<ul>1. Aufbau des Jupyter Notebooks</ul>\n","<ul>2. Instalation und Imports</ul>\n","    <ul>\n","     <ul>2.1. Installationen</ul>\n","     <ul>2.2. Imports</ul>\n","    </ul>\n","<ul>3. Laden der Amazon Daten</ul>\n","    <ul>\n","     <ul>3.1. Entpacken der Zip Files</ul>\n","     <ul>3.2. Laden des Datensatzes in einen Data Frame</ul>\n","    </ul>\n","<ul>4. Data Preperation</ul>\n","    <ul>\n","     <ul>4.1. Lowercasing</ul>\n","     <ul>4.2. Lemmatisierung</ul>\n","     <ul>4.3. Stemming</ul>\n","     <ul>4.4. Entfernung von Satzzeichen</ul>\n","     <ul>4.5. Entfernung von Stopwords</ul>\n","     <ul>4.6. Entfernung von Zahlen</ul>\n","     <ul>4.7. Entfernung von nicht ASCII konformen Wörtern</ul>\n","     <ul>4.8. Entfernung von Links</ul>\n","     <ul>4.9. Tokenization</ul>\n","    </ul>\n","<ul>5. Stoppen der Spark seccion</ul>\n","<ul>6. Literaturverzeichnis</ul>\n","<hr>"]},{"cell_type":"markdown","metadata":{"id":"s_7B_3cyP8A3"},"source":["## 1. Aufbau des Jupyter Notebooks\n","Im Rahmen dieser Masterarbeit wurde versucht, die Beschleunigung der Data Preperation mithilfe von Pyspark zu implementieren, um die Perfomance zu verbessern und die Ladevorgänge zu beschleunigen. \n","\n","PySpark wurde von den Entwicklern des Big-Data-Systems Apache Spark herausgebracht und ist eine Version, welche in der Python Programmierung Anwendung findet. Die Architektur verteilt sich auf ein Cluster, wodurch es ermöglicht wird große Datenmengen zu parallelisieren und performanter zu verarbeiten. Hierfür wird auf den Arbeitsspeicher der Hardware zugegriffen wodurch die Festplatte nicht belastet wird (Wuttke, Einführung in Apache Spark: Komponenten, Vorteile und Anwendungsbereiche, 2022). Es gilt als eines der besten Frameworks in der Datenwissenschaft und verbindet Big Data mit maschinellem Lernen, da es sowohl In-Memory-Operationen für eine bis zu 100-fache Beschleunigung liefert als auch Zusatzpakete wie MLib und GraphX bietet. Spark läuft auf einer Virtuellen Maschine von Java (JVM), ist in Scala geschrieben und auf Hadoop/HDFS implementiert. PySpark ist die Python-API, welche eine Schnittstelle zwischen der Python-Programmierung und dem Spark-Framework liefert. Auf diese Weise ist es möglich mithilfe von Objekten die Vorteile von Spark zu nutzen, ohne die Programmiersprache Scala zu erlernen (Sakar, 2018).\n","\n","Da jedoch für das Anzeigen von Graphiken und das speichern des verarbeiteten Datensatzes die Cloudumgebung von Databricks benötigt wird, wurde dieser Ansatz wieder verworfen. Der vollständigkeit halber ist in diesem Notebook die explorative Auseinandersetzung mit der Data Preperation mithilfe von Pyspark festgehalten.\n","\n","## 2. Instalationen und Imports\n","\n","Um mit Apache Spark in Juyter Notebooks arbeiten zu können, muss die richtige Version zuerst heruntergeladen werden. Hierfür kann auf der Webside von Apache Spark die richtige Version heruntergeladen werden.\n","\n","Darüber hinaus ist es erforderlich Java (JDK) zu installieren. Benötigt wird die Version 8, damit Python in der Lage ist mit der Virtuellen Maschine von Java zu kommunizieren (Preusler, 2020). Für den Download kann direkt die Website von Java aufgerufen werden (Oracle, 2023).\n","Nach dem Download befindet sich die Datei „JavaSetup8u351.exe“ im Download Ordner. Diese muss lediglich in den aktuellen Arbeitsordner kopiert werden. Es ist wichtig, dass sich die Downloads von Spark und Java in dem gleichen Ordner befinden, in welchem das Jupyter Notebook abgelegt ist, damit das Coding fehlerfrei läuft. Nach den genannten Korrekturen konnte die Spark Session erfolgreich gestartet werden.\n","\n","\n","### 2.1. Installationen\n","\n","Nun kann die Library über den „pip install“ Befehl oder über das Terminal installiert werden. In diesem Fall hat die Installation über das Terminal stattgefunden (Shah, 2018)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43615,"status":"ok","timestamp":1680385354152,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"EpixpISkP8A3","outputId":"b1194161-4748-44c1-b43c-ab22720fd8eb"},"outputs":[],"source":["# Instalationen\n","% pip install gzip\n","% pip install shutil\n","% pip install findspark\n","% pip install pyspark\n","% pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"RDgI6t7CP8A4"},"source":["### 2.2. Imports\n","Nach der erfolgreichen Installation von pyspark können die für die Beschleunigung des Ladevorgang benötigten Klassen importiert werden. Wichtig ist jedoch, dass zuvor „findspark“ importiert und initialisiert wurde, damit die benötigte Spark Instanz gewunden wird (Shah, 2018)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1680386617760,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"meuU3Z7VP8A4","outputId":"7759a1fb-fb5d-4d3a-e5bc-d5265d407fc5"},"outputs":[],"source":["# Imports und Initalisierungen\n","import pandas as pd\n","\n","# Progressbar\n","from tqdm.auto import tqdm\n","from tqdm.notebook import tqdm_notebook\n","tqdm_notebook.pandas()\n","\n","# Entpacken der Files\n","import gzip\n","import shutil\n","\n","# Beschleunigung mit Joblib\n","import json\n","import joblib\n","from joblib import Parallel, delayed\n","\n","# Beschleunigung mit Pyspark\n","import findspark\n","findspark.init()\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf, split\n","from pyspark.sql import functions as f\n","from pyspark.sql.types import StringType, ArrayType\n","\n","# Datenverarbeitung\n","import re\n","\n","# Natural Language Processing\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer             # Lemmatisierung zur Textdimensionsreduktion\n","nltk.download('averaged_perceptron_tagger')                 # Für POS-Tagging\n","from nltk import word_tokenize                              # Tokenisierung\n","from nltk import pos_tag                                    # Bestimmung der grammatikalischen Token\n","nltk.download('stopwords')                                  # Herunterladen der Liste mit Stopwords\n","stopwords.words(\"english\")                                  # Sprache der Stopwords\n","from nltk.stem.snowball import SnowballStemmer              # Stemmer zur Textdimensionsreduktion\n","from nltk.corpus import stopwords                           # Zur entfernung der Stopwords"]},{"cell_type":"markdown","metadata":{"id":"QrXTYLYSP8A5"},"source":["## 3. Laden der Amazon Daten\n","\n","### 3.1. Entzippen der Json Files\n","Die Amazon Datensätze sind aufgrund der großen Datenmengen als Zip Dateien gespeichert. Um diese in das Jupyter Notebook einlesen zu können, müssen daher die JSON Files zuerst entpackt werden.\n","\n","Zum Entpacken der Files wird im Folgenden \"gzip\" in Verbindung mit \"shutil\" verwendet. \n","Gzip ist ein Programm zur Daten Kompression (Free Software Foundation, 2022) während shutil ein Modul ist, welches diverse High-Level-Operationen zur Unterstützung beim Kopieren und Löschen von Dateien bietet (Python-Software-Foundation, 2023). \n","Durch deren Kombination werden zuerst zwei Files geöffnet. file_in beschreibt hier das gezippte JSON file und bei file_out handelt es sich um ein leeres JSON File, in welches die Daten aus file_in mithilfe der Funktion \"copyfileobj\" von shutil kopiert werden (Erick, 2018). "]},{"cell_type":"code","execution_count":30,"metadata":{"id":"NdlMVWO_P8A6"},"outputs":[],"source":["# Entpacken der Datei und speichern in einem JSON File\n","# Quelle: https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n","# Review Daten\n","with gzip.open('Sports_and_Outdoors.json.gz', 'rb') as file_in:\n","    with open('Sports_and_Outdoors.json', 'wb') as file_out:\n","        shutil.copyfileobj(file_in, file_out)\n","\n","# Meta Daten\n","with gzip.open('meta_Sports_and_Outdoors.json.gz', 'rb') as file_in:\n","    with open('meta_Sports_and_Outdoors.json', 'wb') as file_out:\n","        shutil.copyfileobj(file_in, file_out)"]},{"cell_type":"markdown","metadata":{"id":"6I1nVnTWP8A6"},"source":["### 3.2. Laden des Datensatzes in einen Data Frame\n","\n","Nach dem Erfolgreichen Abschluss aller Installationen und Imports, sowie dem Entpacken der Files, kann als nächstes eine Spark Session mit folgendem Befehl gestartet werden (Shah, 2018):"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":9279,"status":"ok","timestamp":1680385416145,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"TDgHgGzuP8A6","outputId":"f7d940d8-a1d3-4d67-a5e4-213c2bf5ab02"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://DESKTOP-4EQA5DC:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x1e247168310>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Starten der Spark Session\n","# https://medium.com/@ashish1512/how-to-setup-apache-spark-pyspark-on-jupyter-ipython-notebook-3330543ab307\n","spark = SparkSession.builder.getOrCreate()\n","spark"]},{"cell_type":"markdown","metadata":{"id":"nsci_ZoNP8A6"},"source":["Aufgrund der verschachtelten Struktur des Datensatzes kommt es dazu, dass Duplikate in den Spaltennamen vorhanden sind. Dies ist der Fall, da mehrere Spalten mit dem Wort „style“ beginnen. Um diesen Fehler zu umgehen, kann die Spark Session konfiguriert werden, indem die Berücksichtigung der Groß- und Kleinschreibung aktiviert wird (shoeboxer, 2015):"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680385416146,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"5o_aPKNRP8A6"},"outputs":[],"source":["# Konfiguration \"Case Sensitive\" zur Normalisierung\n","# Quelle: https://stackoverflow.com/questions/33816481/duplicate-columns-in-spark-dataframe\n","spark.conf.set(\"spark.sql.caseSensitive\", \"true\")"]},{"cell_type":"markdown","metadata":{"id":"7RYY7330P8A7"},"source":["Mit der folgenden Funktion wird der Datensatz in eine Data Frame laden. Durch die Methode \"format\" lässt sich die Datenquelle der Datei weiter spezifizieren. (SparkBy{Examples}, 2020)."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":126624,"status":"ok","timestamp":1680385566526,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"S1NhjNZhP8A7"},"outputs":[],"source":["# Laden des Json Files  der Meta Data in einen Spark Data Frame\n","# Quelle: https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline?utm_content=cmp-true\n","meta_df = spark.read.format('org.apache.spark.sql.json').load(\"meta_Sports_and_Outdoors.json\")"]},{"cell_type":"markdown","metadata":{},"source":["Nachdem die Meta Daten geladen wurden, werden nun die Review Daten eingespielt. Um die Auswirkungen der Beschleunigung besser darstellen zu können, wurde der Ladevorgang mit Joblib wiederholt. Die Progressbars zeigen einen deutlichen Unterschied der Geschwindigkeit der Ladevorgänge."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0a865fe68e946e6ba0ecee78ebd2104","version_major":2,"version_minor":0},"text/plain":["Laden von 12.980.837 Zeilen mithilfe von Pyspark:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"850c22b8734b4c26aa5865b0cd4f6778","version_major":2,"version_minor":0},"text/plain":["Laden von 12.980.837 Zeilen mithilfe von Joblib:   0%|          | 0/12980837 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba45ee2887f742a9bfb598fbf39b0230","version_major":2,"version_minor":0},"text/plain":["Laden von 12.980.837 Zeilen mithilfe von Joblib:   0%|          | 0/12980837 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Laden der Review Daten mithilfe von Pyspark\n","# Quelle: https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline?utm_content=cmp-true\n","for i in tqdm(range(1), desc =\"Laden von 12.980.837 Zeilen mithilfe von Pyspark: \"):\n","    review_df = spark.read.format('org.apache.spark.sql.json').load(\"Sports_and_Outdoors.json\")\n","\n","# Laden der Review Daten mithilfe von Joblib\n","r = open('Sports_and_Outdoors.json')\n","dataset_review = Parallel(n_jobs=-1)(delayed(json.loads)(line) for line in tqdm(r,\n","                                                                                desc =\"Laden von 12.980.837 Zeilen mithilfe von Joblib: \",\n","                                                                                total = 12980837))"]},{"cell_type":"markdown","metadata":{"id":"9Xfjzu4tP8A_"},"source":["## 4. Datapreparation\n","\n","Um einen Konsistenten Datensatz für die weitere Verarbeitung und die Anwendung der Topic Modelling Modelle zu erhalten, werden die oben aufgeführten Bereinigungen durchgeführt.\n","\n","Doch befor die Textbereinigung stattfindet, werden alle Zeilen, welche in den reviewText keine Werte enthalten gelöscht.\n","Darüber hinaus werden die Spalten des Data Frame reduziert, sodass lediglich diese Spalten für die Modellierung benötigt werden und die Performance durch die verringerung der Daten verbessert werden kann."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"A8xAh772P8A_"},"outputs":[],"source":["# Neuer Data Frame, welcher die relevanten Spalten in englischer Sprache beinhaltet\n","text_df = review_df.select(\"reviewText\", \"reviewTime\", \"asin\", \"overall\")\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame (wird hier nicht benötigt)\n","text_df = text_df.where(text_df.reviewText.isNotNull())"]},{"cell_type":"markdown","metadata":{"id":"757CHmG6P8A_"},"source":["### 4.1. Lowercasing\n","\n","Mithilfe des Lowercasings werden alle Großbuchstaben in Kleinbuchstaben konvertiert, da diese für das Verständnis der Texte nicht benötigt werden. Hierfür wird zuerst eine Funktion definiert, welche einen string in Kleinbuchstaben konvertiert (Pomer, 2022).\n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert.\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis des Lowercasing lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"6J13qNqLP8BA"},"outputs":[],"source":["# Definition der Funktion, um alle Buchstaben in Kleinschreibung zu konvertieren\n","# Quelle: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","def to_lower(in_string):\n","    out_string = in_string.lower()\n","    return out_string"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ypL6Qq1rP8BA"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(reviewText='What a spectacular tutu! Very slimming.', text_lower='what a spectacular tutu! very slimming.'), Row(reviewText='What the heck? Is this a tutu for nuns? I know you can cut it but STILL. Also there aren\\'t several layers of the tutu making it \"poof out\" It just lays flat. Needless to say it was returned.', text_lower='what the heck? is this a tutu for nuns? i know you can cut it but still. also there aren\\'t several layers of the tutu making it \"poof out\" it just lays flat. needless to say it was returned.')]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","to_lower_udf = udf(to_lower, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_lower\", to_lower_udf(text_df[\"reviewText\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame (wird hier nicht benötigt)\n","text_df = text_df.where(text_df.text_lower.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"reviewText\", \"text_lower\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"Mia6UXR7P8BA"},"source":["### 4.2. Lemmatisierung\n","Bei der Lemmatisierung handelt es sich um die Umwandlung von Wörtern in ihre Grundform unter Verwendung von Wörterbüchern (Pomer, 2022). Hierbei werden Pluralformen in Singularformen und unterschiedliche Zeitformen in das Präsenz umgewandelt (Yuzhe-Lu, 2018). Für die Lemmatisierung wird eine Funktion definiert, welche die Sätze in einzelen Token aufsplittet, desse POS-Tags ermittelt und basierend dieser Tags die Lemmatisierung mithilfe der Funktion \"lemmatize()\" durchführt (Johnson, 2023).  \n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert.\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis der Lemmatisierung lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"jTGKZE7sP8BA"},"outputs":[],"source":["# Definition der Funktion zur Lemmatisierung, um Wörter in ihre Grundform zu konvertieren\n","# Quelle 1: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","# Quelle 2: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#stanfordcorenlplemmatization\n","\n","def lemmatize(in_string):\n","    # Definition der notwendigen Parameter\n","    list_pos = 0                                  # Zuordnung einer Positionsnummer\n","    cleaned_str = ''                              # Leerer String für bereinigte Wörter\n","    text_token = nltk.word_tokenize(in_string)    # Tokenisieren der Sätze in einzelne Strings\n","    tagged_words = pos_tag(text_token)            # Grammatikalisches Tagging\n","    wnl = WordNetLemmatizer()                     # Klasse von NLTK für Lemmatisierung\n","    \n","    # Durchfürhung der Lemmatisation und Zusammenführung der Ergebnisse in einen String\n","    for word in tagged_words:\n","        if 'v' in word[1].lower():\n","            lemma = wnl.lemmatize(word[0], pos='v')\n","        else:\n","            lemma = wnl.lemmatize(word[0], pos='n')\n","        if list_pos == 0:\n","            cleaned_str = lemma\n","        else:\n","            cleaned_str = cleaned_str + ' ' + lemma\n","        list_pos += 1\n","    return cleaned_str"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2joK61R9P8BA"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_lower='what a spectacular tutu! very slimming.', text_lemmatize='what a spectacular tutu ! very slimming .'), Row(text_lower='what the heck? is this a tutu for nuns? i know you can cut it but still. also there aren\\'t several layers of the tutu making it \"poof out\" it just lays flat. needless to say it was returned.', text_lemmatize=\"what the heck ? be this a tutu for nun ? i know you can cut it but still . also there be n't several layer of the tutu make it `` poof out '' it just lay flat . needle to say it be return .\")]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","lemmatize_udf = udf(lemmatize, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_lemmatize\", lemmatize_udf(text_df[\"text_lower\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_lemmatize.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_lower\", \"text_lemmatize\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"2Y21owjkP8BB"},"source":["### 4.3. Stemming\n","\n","Da die Lemmatisierung nicht alle Worte bereinigt hat, wie es zum Beispiel bei dem Wort \"quickly\" zu erkennen ist, welches eigentlich in \"quick\" konvertiert werden sollte, wird im folgenden zusätzlich der sogenannte Snowball-Stemming-Algorithmus implementiert.\n","\n","Im Gegensatz zur Lemmatisierung, beruft sich das Stemming nicht auf Wörterbücher, sondern entfernt die Sufixe und Präfixe der Wörter. Es findet daher kein Bezug auf den Kontext der Wörter statt\n","(Luber & Litzel, Was ist Stemming?, 2020).\n","\n","Zur Durchführung des Stemming wird eine Funktion definiert, welche die Sätze zuerst in einzelne Token unterteilt und den Snowball Stemmer auf diese mithilfe einer For-Schleife anwendet und wieder zu einem Satz verbindet (NLTK Project, 2023).\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert. Darüber hinaus werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis des Stemmings lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"GPOJlClUP8BB"},"outputs":[],"source":["# Definition der Funktion zur Durchführung des Stemmings\n","# Quelle: https://www.nltk.org/howto/stem.html\n","def stemming(in_string):\n","    # Definition der notwendigen Parameter\n","    text_token = nltk.word_tokenize(in_string)      # Tokenisieren der Sätze in einzelne Strings\n","    stemmer = SnowballStemmer(\"english\")            # Laden der Klasse zum Stemming\n","    \n","    # Durchfürhung des Stemmings und Zusammenführung der Ergebnisse in einen String\n","    cleaned_str = ' '.join([stemmer.stem(word) for word in text_token])\n","    return cleaned_str"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"iKTlsGRKP8BB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_lemmatize='what a spectacular tutu ! very slimming .', text_stemming='what a spectacular tutu ! veri slim .'), Row(text_lemmatize=\"what the heck ? be this a tutu for nun ? i know you can cut it but still . also there be n't several layer of the tutu make it `` poof out '' it just lay flat . needle to say it be return .\", text_stemming=\"what the heck ? be this a tutu for nun ? i know you can cut it but still . also there be n't sever layer of the tutu make it `` poof out `` it just lay flat . needl to say it be return .\")]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","stemming_udf = udf(stemming, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_stemming\", stemming_udf(text_df[\"text_lemmatize\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_stemming.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_lemmatize\", \"text_stemming\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"7riSNMCAP8BB"},"source":["### 4.4. Entfernung von Satzzeichen\n","\n","Nachdem die bereinigung der Wörter selbst abgeschlossen ist, werden nun die Satzzeichen aus dem Datensatz entfernt. Viele Satzzeichen verändern bei ihrem Fehlen den Kontext nicht groß, sodass diese problemlos bereinigt werden können. Jedoch ist zu beachten, dass beispielsweise Kommata oder Fragezeichen den Inhalt eines Textes verändern können, sodass es bei der Analyse zu Fehlinterpretationen kommt. Besonders zu beachten sind in diesem Zusammenhang Zahlen, da es bei dem Jahr \"1998\" und einem Geldwert \"19,98\" einen großen Unterschied macht, ob das Komma im Datensatz erhalten bleibt oder ob es verschwindet. Bei letzteren Fall hätten beide Zahlen den identischen Wert und das Ergebnis der Auswertung hätte keine Aussagekraft (Pomer, 2022). Aus diesem Grund soll im Folgenden mit diesem Wissen im Hinterkopf eine Bereinigung der Satzzeichen stattfinden. Treten bei dem Ergebnis der Analyse vermehrt Zahlen oder andere Unstimmigkeiten auf, müssen die Satzzeichen ggf. in die Modelle miteinbezogen werden.\n","\n","Für die Bereinigung werden folgende Zeichen durch einen leeren Wert mithilfe der PySpark functions \"col()\" und translate()\" ersetzt. Der Import der functions ist in Abschnitt 1. Installationen und Imports zu finden (Apache Spark, kein Datum):\n","\n","> '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n","\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"cJ-0JmbVP8BB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_stemming='what a spectacular tutu ! veri slim .', text_punctation='what a spectacular tutu  veri slim '), Row(text_stemming=\"what the heck ? be this a tutu for nun ? i know you can cut it but still . also there be n't sever layer of the tutu make it `` poof out `` it just lay flat . needl to say it be return .\", text_punctation='what the heck  be this a tutu for nun  i know you can cut it but still  also there be nt sever layer of the tutu make it `` poof out `` it just lay flat  needl to say it be return ')]\n"]}],"source":["# Entfernung der Satzzeichen mit Hilfe von PySpark\n","# Quelle: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n","text_df = (text_df.withColumn(\"text_punctation\", f.translate(f.col(\"text_stemming\"), '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', '')))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_punctation.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_stemming\", \"text_punctation\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"_fFtvo4kP8BC"},"source":["### 4.5. Entfernung von Stopwords\n","\n","Stoppwörter können in Vorbereitung auf das Topic Modelling ebenfalls herausgefiltert werden, da diese keinen Einfluss auf die Bedeutung des Kontextes haben. Aufgrund ihrer Häufigkeit in einem Text wird darüber hinaus der Datensatz drastisch reduziert, wordurch die Laufzeit sowie die Genauigkeit der Analyse verbessert werden kann. Die Auswahl der Stopword muss jedoch bedacht getroffen werden, um die Ergebnisse nicht zu verfälschen. So ist beispielsweise die Negation \"nicht\" in vielen Sprachen als Stopword eingestuft, kann jedoch den Kontext eines Satzes bei dessen Entfernung dramatisch verändern (Teja, 2020). Für jede Sprache existieren unterschiedliche Stoppwörter. Um diese aus den Daten herauszufiltern werden sogenannte Stoppwordlisten herangezogen (Pomer, 2022).\n","\n","Für die Bereinigung der Daten im Rahmen dieser Masterarbeit wird eine Liste der englischen Stopword der NLTK Library verwendet. \n","Zur Durchführung der Bereinigung wird eine Funktion definiert, welche die Token des Amazon Datensatzes mit der Liste der Stopwords vergleicht und einen bereinigten String kreiert, welcher keine Stopwords mehr enthält (Yuzhe-Lu, 2018).\n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"kSXjSm9mP8BC"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der Stopwords\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","def remove_stops(in_string):\n","    # Definition der notwendigen Parameter\n","    list_pos = 0                                  # Zuordnung einer Positionsnummer\n","    cleaned_str = ''                              # Leerer String für bereinigte Wörter\n","    text_token = nltk.word_tokenize(in_string)    # Tokenisieren der Sätze in einzelne Strings\n","    stop_words = stopwords.words('english')       # Bestimmen der Stopwords (für Englisch)\n","    stop_words.append('would')                    # Hinzufügen individueller Wörter zur Liste der Stopwords\n","\n","    # Durchfürhung der Entfernung der Stopwords und Zusammenführung der Ergebnisse in einen String\n","    for word in text_token:\n","        if word not in stop_words:\n","            if list_pos == 0:\n","                cleaned_str = word\n","            else:\n","                cleaned_str = cleaned_str + ' ' + word\n","            list_pos += 1\n","    return cleaned_str"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"LHuZAdjeP8BC"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_punctation='what a spectacular tutu  veri slim ', text_stopwords='spectacular tutu veri slim'), Row(text_punctation='what the heck  be this a tutu for nun  i know you can cut it but still  also there be nt sever layer of the tutu make it `` poof out `` it just lay flat  needl to say it be return ', text_stopwords='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return')]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","# Quelle: https://www.legendu.net/misc/blog/spark-issue:-_pickle.PicklingError:-args[0]-from-__newobj__-args-has-the-wrong-class/\n","\n","# setup PySpark udf Funktion\n","remove_stops_udf = udf(remove_stops, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_stopwords\", remove_stops_udf(text_df[\"text_punctation\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_stopwords.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_punctation\", \"text_stopwords\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"uDdHEQ3SP8BC"},"source":["### 4.6. Entfernung von Zahlen\n","\n","Im Rahmen der Masterarbeit wurde beschlossen, die Zahlen aus dem Datensatz zu entfernen, da die Annahme getroffen wurde, dass diese nicht zur Einteilung in die richtigen Kategorien beitragen.\n","Mithilfe der Regular expressions werden die Nummern 0 bis 9 in einerm Parameter gespeichert. Dieser wird daraufhin mit dem Input Text abgeglichen und bei einer Übereinstimmung wird die Zahl mit \"nichts\" ausgetauscht (Python Software Foundation, 2023). \n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"e4M50rwDP8BC"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der Zahlen\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","# Quelle: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","def remove_numbers(in_string):\n","    # Kompilieren der Zahlen 0-9 in ein Ausdrucksmuster\n","    num_re = re.compile('(\\\\d+)')\n","\n","    # Ersetzen der Zahlen durch \"nichts\"\n","    cleaned_str = \" \".join((re.sub(num_re, \"\", in_string)).split()) \n","    return cleaned_str"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"KBPBLfuvP8BD"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_stopwords='spectacular tutu veri slim', text_numbers='spectacular tutu veri slim'), Row(text_stopwords='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return', text_numbers='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return')]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","remove_numbers_udf = udf(remove_numbers, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_numbers\", remove_numbers_udf(text_df[\"text_stopwords\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_numbers.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_stopwords\", \"text_numbers\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"PfreefaqP8BD"},"source":["### 4.7. Entfernung von nicht ASCII konvormen Wörtern\n","\n","ASCII ist die Kurzform für „American Standard Code for Information Interchange“ und dient zur standartisierten Darstellung von Zeichen in elektronischer Form (IONOS SE, 2022). Zu beachten ist, dass es bei diesem um einen amerikanischen Standart handelt und daher keine Sonderzeichen, wie „ß“ oder „é, á“ berücksichtigt und daher beispielsweise bei deutschen Datensätzen das ergebnis verfälschen könnte (Pomer, 2022). Da der Amazondatensatz zufor auf die Sprache geprüft und bereinigt wurde, beinhaltet dieser lediglich englische Sätze, wodurch der ASCII Standard verwendet werden kann.\n","\n","Zur Bereinigung wird die untenstehende Funktion definiert, welche den Unicode der wörter ermittelt und diese mit den Unicodes aus dem ASCII Standard abgleicht. Lediglich die Wörter, welche zwischen 0 und 128 liegen werden im Datensatz beibehalten.\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"r0_cPu-UP8BD"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der ASCII characters\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","def remove_non_ascii_words(in_string):\n","    ''' Returns the string without non ASCII characters'''\n","    cleaned_str = ''.join(word for word in in_string if 0 < ord(word) < 128)\n","    return cleaned_str"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"jVMTTIGaP8BD"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_numbers='spectacular tutu veri slim', text_ascII='spectacular tutu veri slim'), Row(text_numbers='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return', text_ascII='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return')]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","remove_non_ascii_words_udf = udf(remove_non_ascii_words, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_ascII\", remove_non_ascii_words_udf(text_df[\"text_numbers\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_ascII.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_numbers\", \"text_ascII\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"zkD4MJzAP8BE"},"source":["### 4.8. Entfernung von Webside Links und Mentions\n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"Gp3yzxkyP8BE"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der Hyperlinks, html-Symbolden und Erwähnungen\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","def remove_features(in_string):\n","    # Festlegung der Bedingungen\n","    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n","    html_re = re.compile(\"<br />\")\n","    mention_re = re.compile('@(\\w+)')\n","\n","    # Ersetzen der definierten Bedingungen durch \" \"\n","    cleaned_string = url_re.sub(' ', in_string)         # Entfernung von Hyperlinks\n","    cleaned_string = html_re.sub(' ', in_string)        # Entfernung der html Symbole    \n","    cleaned_string = mention_re.sub(' ', in_string)     # Rentfernen von Erwähnungen mit \"@mentions\"\n","\n","    return cleaned_string"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"jgfN-pe-P8BE"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_numbers='spectacular tutu veri slim', text_ascII='spectacular tutu veri slim'), Row(text_numbers='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return', text_ascII='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return')]\n"]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","remove_features_udf = udf(remove_features, StringType())\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"text_features\", remove_features_udf(text_df[\"text_ascII\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.text_features.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_numbers\", \"text_ascII\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"jVvw4YnaP8BE"},"source":["### 4.9. Tokenize"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"kZQNr1yWP8BE"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(text_features='spectacular tutu veri slim', token_text=['spectacular', 'tutu', 'veri', 'slim']), Row(text_features='heck tutu nun know cut still also nt sever layer tutu make `` poof `` lay flat needl say return', token_text=['heck', 'tutu', 'nun', 'know', 'cut', 'still', 'also', 'nt', 'sever', 'layer', 'tutu', 'make', '``', 'poof', '``', 'lay', 'flat', 'needl', 'say', 'return'])]\n"]}],"source":["# Anwenden des Tokenizers auf den DataFrame\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","\n","# setup PySpark udf Funktion\n","tokenize_udf = udf(word_tokenize, ArrayType(StringType()))\n","\n","# Ergänzen des Data Frames mit Spalte mit beneinigten Daten\n","text_df = text_df.withColumn(\"token_text\", tokenize_udf(text_df[\"text_features\"]))\n","\n","# Herausfiltern der leeren \"non-type values\" aus den Data Frame\n","text_df = text_df.where(text_df.token_text.isNotNull())\n","\n","# Ausgabe (Vergleich des unbereinigten und bereinigten Daten)\n","print(text_df.select(\"text_features\", \"token_text\").take(2))"]},{"cell_type":"markdown","metadata":{"id":"U7gcRdOpP8BF"},"source":["### 4.10. Merge mit Metadaten und Selection der Spalten\n"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"-k2PUjWMP8BF"},"outputs":[],"source":["# Merge der Meta Daten mit dem bereinigeten Datensatz\n","merged_df = text_df.join(meta_df, text_df.asin == meta_df.asin, 'left')\n","\n","# Neuen Data Frame erzeugen, welcher nur die relevanten Spalten enhält\n","cleaned_data_df = merged_df.select(\"token_text\", \"reviewTime\", \"overall\", \"brand\", \"price\", \"title\")"]},{"cell_type":"markdown","metadata":{"id":"z3u8WV7eP8BF"},"source":["## 5. Stoppe der Spark Session"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"tvFv2HO7P8BF"},"outputs":[],"source":["# Spark Session deaktivieren\n","spark.stop()"]},{"cell_type":"markdown","metadata":{"id":"PKgkXH0wP8BF"},"source":["## 6. Literaturverteichnis\n","\n","Apache Spark. (17. 02 2023). Download Apache Spark™. Abgerufen am 20. 02 2023 von spark.apache.org: https://spark.apache.org/downloads.html\n","\n","Erick. (12. 02 2018). How to unzip gz file using Python. Abgerufen am 24. 10 2022 von stackoverflow.com: https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n","\n","Free Software Foundation, I. (02. 04 2022). GNU Gzip: General file (de)compression. Abgerufen am 20. 02 2023 von gnu.org: https://www.gnu.org/software/gzip/manual/gzip.html\n","\n","IONOS SE. (05. 04 2022). ASCII – Erklärung und Beispiele. Abgerufen am 08. 03 2023 von ionos.de: https://www.ionos.de/digitalguide/server/knowhow/ascii-american-standard-code-for-information-interchange/\n","\n","Johnson, D. (21. 01 2023). POS-Tagging mit NLTK und Chunking in NLP [BEISPIELE]. Abgerufen am 21. 02 2023 von guru99.com: https://www.guru99.com/pos-tagging-chunking-nltk.html#1\n","\n","Luber, D.-I. & Litzel, N. (30. 11 2020). Was ist Stemming? Abgerufen am 22. 02 2023 von bigdata-insider.de: https://www.bigdata-insider.de/was-ist-stemming-a-980852/#:~:text=Abgrenzung%20von%20Stemming%20und%20Lemmatisierung&text=Stemming%2DAlgorithmen%20arbeiten%20meist%20mit,)%2C%20um%20Stammformen%20zu%20finden.\n","\n","NLTK Project. (02. 01 2023). Natural Language Toolkit - Sample usage for stem. Abgerufen am 22. 02 2023 von nltk.org: https://www.nltk.org/howto/stem.html\n","\n","Oracle. (17. 01 2023). Download Java for Windows. Abgerufen am 20. 02 2023 von java.com: https://www.java.com/download/ie_manual.jsp\n","\n","Pomer, L. (03. 08 2022). Textklassifikation – Vorverarbeitung der Daten. Abgerufen am 14. 02 2023 von thecattlecrew.net: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","\n","Preusler, S. (13. 03 2020). Pyspark und Jupyter Notebook Anleitung für Windows. Abgerufen am 14. 01 2023 von https://medium.com/@stefan.preusler/pyspark-und-jupyter-notebook-anleitung-f%C3%BCr-windows-7317f2c968c4\n","\n","Python Software Foundation. (2023). re — Regular expression operations¶. Abgerufen am 08. 03 2023 von docs.python.org: https://docs.python.org/3/library/re.html\n","\n","Shah, A., 2018. How to setup Apache Spark(PySpark) on Jupyter/IPython Notebook?. [Online] \n","Available at: https://medium.com/@ashish1512/how-to-setup-apache-spark-pyspark-on-jupyter-ipython-notebook-3330543ab307\n","[Accessed 14 01 2023].\n","\n","Sarkar, T. (12. 11 2018). How to set up PySpark for your Jupyter notebook. Abgerufen am 14. 01 2023 von opensource.com: https://opensource.com/article/18/11/pyspark-jupyter-notebook\n","\n","SparkBy{Examples}. (06. 12 2020). PySpark Liest JSON-Datei in DataFrame. Abgerufen am 14. 01 2023 von https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline\n","\n","Teja, S. (10. 06 2020). Stop Words in NLP. Abgerufen am 24. 02 2023 von medium.com: https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\n","\n","Wuttke, L. (2022). Einführung in Apache Spark: Komponenten, Vorteile und Anwendungsbereiche. Abgerufen am 05. 01 2023 von datasolut.com: https://datasolut.com/was-ist-spark/#was-ist-pyspark\n","\n","Yuzhe-Lu. (10. 12 2018). Topic-Modeling-on-Amazon-Reviews-using-LDA/2_LDA_Data_Processing.ipynb. Abgerufen am 14. 02 2023 von https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb"]}],"metadata":{"colab":{"collapsed_sections":["V7GIDkK7P8A1","uSfhN2M6P8A7","-TFWdcL5P8A8","i3qdIqPUP8A8","jLdys3KNP8A9","g2mkmZnrP8A-","e-uyPtj6P8A-","OnSQCQVaP8A-","skfU9c9mP8A_","6yBWS5TyP8A_","oKZnPIpCoRFN","AQlHynY2oUWW","Wbdo9EOdoXt0","aG7sXbVjoaCU","757CHmG6P8A_","Mia6UXR7P8BA","2Y21owjkP8BB","7riSNMCAP8BB","_fFtvo4kP8BC","uDdHEQ3SP8BC","PfreefaqP8BD","zkD4MJzAP8BE","jVvw4YnaP8BE","U7gcRdOpP8BF","oHZ2e79BP8BF","z3u8WV7eP8BF","PKgkXH0wP8BF"],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4aa37d1a4c10a2e7b72a0d527e28b9c54c47ddea71f47cc08f2d095dc3868839"}}},"nbformat":4,"nbformat_minor":0}
