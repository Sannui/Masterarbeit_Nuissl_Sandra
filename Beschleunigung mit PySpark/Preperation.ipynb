{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"V7GIDkK7P8A1"},"source":["### Nuißl Sandra, 14.08.2023\n","## \"Empirische Evaluation von ‚State Of The Art‘ Topic Modeling Ansätze am Beispiel von Produktreviews für die Entscheidungsunterstützung in Unternehmen\"\n","### - Data Analyse und Data Preperation -\n","<center><img src=\"Titelbild.jpg\" height=\"300px\" width=\"1100px\"/></center>\n","<center><font size=\"1\">https://www.cloudways.com/blog/wp-content/uploads/Product-Review-1024x576.jpg</font></center>\n","\n","\n","<hr>\n","\n","## **Inhaltsverzeichnis**\n","\n","<ul>1. Aufbau des Jupyter Notebooks</ul>\n","<ul>2. Instalation und Imports</ul>\n","    <ul>\n","     <ul>2.1. Installationen</ul>\n","     <ul>2.2. Imports</ul>\n","    </ul>\n","<ul>3. Laden der Amazon Daten</ul>\n","    <ul>\n","     <ul>3.1. Entpacken der Zip Files</ul>\n","     <ul>3.2. Laden des Datensatzes in einen Data Frame</ul>\n","    </ul>\n","<ul>4. Data Preperation</ul>\n","    <ul>\n","     <ul>4.1. Lowercasing</ul>\n","     <ul>4.2. Lemmatisierung</ul>\n","     <ul>4.3. Stemming</ul>\n","     <ul>4.4. Entfernung von Satzzeichen</ul>\n","     <ul>4.5. Entfernung von Stopwords</ul>\n","     <ul>4.6. Entfernung von Zahlen</ul>\n","     <ul>4.7. Entfernung von nicht ASCII konformen Wörtern</ul>\n","    </ul>\n","<ul>5. Literaturverzeichnis</ul>\n","<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s_7B_3cyP8A3"},"source":["## 1. Aufbau des Jupyter Notebooks\n","Dieses Jupyter Notebook ist Teil der Master Thesis \"Empirische Evaluation von ‚State Of The Art‘ Topic Modeling Ansätze am Beispiel von Produktreviews für die Entscheidungsunterstützung in Unternehmen\".\n","Es setzt sich zusammen aus der Datenanalyse der Amazon Daten, sowie der Data Preperation. Die dient als Vorbereitung für die spätere Implementierung und Evaluierung der Topic Modelling Modelle.\n","\n","Die theoretischen Inhalte zu diesem Notebook sind der schriftlichen Ausarbeitung dieser Masterarbeit zu entnehmen.\n","\n","## 2. Instalationen und Imports\n","### 2.1. Installationen"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43615,"status":"ok","timestamp":1680385354152,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"EpixpISkP8A3","outputId":"b1194161-4748-44c1-b43c-ab22720fd8eb"},"outputs":[],"source":["# Instalationen\n","#% pip install gzip\n","#% pip install shutil\n","#% pip install nltk\n","#%pip install LanguageIdentifier"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RDgI6t7CP8A4"},"source":["### 2.2. Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1680386617760,"user":{"displayName":"Sandra Nuißl","userId":"08611353544757223818"},"user_tz":-120},"id":"meuU3Z7VP8A4","outputId":"7759a1fb-fb5d-4d3a-e5bc-d5265d407fc5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\nuiss\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Imports und Initalisierungen\n","import datetime\n","import random\n","import numpy as np\n","import pandas as pd\n","from pandas import json_normalize\n","\n","# Progressbar\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm_notebook\n","from tqdm.auto import tqdm\n","tqdm_notebook.pandas()\n","\n","# Entpacken der Files\n","import gzip\n","import shutil\n","\n","# Beschleunigung\n","import json\n","import joblib\n","from joblib import Parallel, delayed\n","\n","\n","# Data Preprocessing\n","from langid.langid import LanguageIdentifier, model        # Ermittlung der Sprache von Texten\n","from LanguageIdentifier import predict\n","\n","# Text\n","import re\n","import string\n","from string import punctuation\n","\n","# Natural Language Processing\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer             # Lemmatisierung zur Textdimensionsreduktion\n","#nltk.download('averaged_perceptron_tagger')                 # Für POS-Tagging\n","#from nltk import word_tokenize                              # Tokenisierung\n","from nltk import pos_tag                                    # Bestimmung der grammatikalischen Token\n","from nltk.stem.snowball import SnowballStemmer              # Stemmer zur Textdimensionsreduktion\n","from nltk.corpus import stopwords                           # Zur entfernung der Stopwords\n","stopwords.words(\"english\")                                  # Festlegung der Stoppwords\n","#nltk.download('punkt')                                      # Für den Export in ein Parquet File\n","#nltk.download('wordnet')                                    # Für den Export in ein Parquet File\n","nltk.download('stopwords')                                  # Herunterladen der Liste mit Stopwords\n","\n","\n","# Datenvisualisierung\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","from wordcloud import WordCloud\n","from PIL import Image"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QrXTYLYSP8A5"},"source":["## 3. Laden der Amazon Daten\n","\n","### 3.1. Entzippen der Json Files\n","Die Amazon Datensätze sind aufgrund der großen Datenmengen als Zip Dateien gespeichert. Um diese in das Jupyter Notebook einlesen zu können, müssen daher die JSON Files zuerst entpackt werden.\n","\n","Zum Entpacken der Files wird im Folgenden \"gzip\" in Verbindung mit \"shutil\" verwendet. \n","Gzip ist ein Programm zur Daten Kompression (Free Software Foundation, 2022) während shutil ein Modul ist, welches diverse High-Level-Operationen zur Unterstützung beim Kopieren und Löschen von Dateien bietet (Python-Software-Foundation, 2023). \n","Durch deren Kombination werden zuerst zwei Files geöffnet. file_in beschreibt hier das gezippte JSON file und bei file_out handelt es sich um ein leeres JSON File, in welches die Daten aus file_in mithilfe der Funktion \"copyfileobj\" von shutil kopiert werden (Erick, 2018). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdlMVWO_P8A6"},"outputs":[],"source":["# Entpacken der Datei und speichern in einem JSON File\n","# Quelle: https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n","# Review Daten\n","#with gzip.open('Sports_and_Outdoors_5.json.gz', 'rb') as file_in:\n","#    with open('Sports_and_Outdoors_5.json', 'wb') as file_out:\n","#        shutil.copyfileobj(file_in, file_out)\n","\n","# Meta Daten\n","#with gzip.open('meta_Sports_and_Outdoors.json.gz', 'rb') as file_in:\n","#    with open('meta_Sports_and_Outdoors.json', 'wb') as file_out:\n","#        shutil.copyfileobj(file_in, file_out)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6I1nVnTWP8A6"},"source":["### 3.2. Laden des Datensatzes in einen Data Frame\n","\n","Nachdem die Datensätze entpackt wurden, können diese geladen in einen Data Frame gespeichert werden."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2566a463af24ef59043abf5c827ca7b","version_major":2,"version_minor":0},"text/plain":["Status der geladenen Zeilen:   0%|          | 0/2839940 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Review Daten\n","# Öffnen der Datei und speichern als Objekt in der Variable f\n","r = open('Sports_and_Outdoors_5.json')\n","\n","# Laden des JSON Files in das Jupyter Notebook mit Beschleunigung\n","dataset_review = Parallel(n_jobs=-1)(delayed(json.loads)(line) for line in tqdm(r, desc =\"Status der geladenen Zeilen: \", total = 2839940))\n","\n","# Normalisieren der Daten und Laden in einen Data Frame\n","review_df = json_normalize(dataset_review)\n","\n","# Ausgabe\n","#pd.concat([review_df], axis=1, sort = False, keys = [\"Review Datensatz: Sports and Outdoors\"])"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04d4590f652b4a0193ba5661c88fc96f","version_major":2,"version_minor":0},"text/plain":["Status der geladenen Zeilen:   0%|          | 0/962300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Meta Daten\n","# Öffnen der Datei und speichern als Objekt in der Variable f\n","m = open('meta_Sports_and_Outdoors.json')\n","\n","# Laden des JSON Files in das Jupyter Notebook mit Beschleunigung\n","dataset_meta = Parallel(n_jobs=-1)(delayed(json.loads)(line) for line in tqdm(m, desc =\"Status der geladenen Zeilen: \", total = 962300))\n","\n","# Normalisieren der Daten und Laden in einen Data Frame\n","meta_df = json_normalize(dataset_meta)\n","\n","# Ausgabe\n","#pd.concat([meta_df], axis=1, sort = False, keys = [\"Meta Datensatz: Sports and Outdoors\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pzFueN6DP8A7"},"source":["## 4. Allgemeine Bereinigungen\n","\n","### 4.1. Selectieren der Spalten und Merge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Erstellen eines neuen DataFrames mit den relevanten Spalten\n","review_df = review_df[[\"reviewText\", \"asin\", \"overall\", \"unixReviewTime\", \"reviewerID\"]]\n","meta_df = meta_df[[\"asin\", ]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Merge der Data Frames\n","# Merge der Meta Daten mit dem bereinigeten Datensatz\n","merged_df = review_df.merge(meta_df, on=\"asin\", how='left')"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2. Nan-Values und doppelte Werte"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviewText</th>\n","      <th>asin</th>\n","      <th>overall</th>\n","      <th>unixReviewTime</th>\n","      <th>reviewerID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What a spectacular tutu! Very slimming.</td>\n","      <td>0000032034</td>\n","      <td>5.0</td>\n","      <td>1433289600</td>\n","      <td>A180LQZBUWVOLF</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What the heck? Is this a tutu for nuns? I know...</td>\n","      <td>0000032034</td>\n","      <td>1.0</td>\n","      <td>1427846400</td>\n","      <td>ATMFGKU5SVEYY</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Exactly what we were looking for!</td>\n","      <td>0000032034</td>\n","      <td>5.0</td>\n","      <td>1421107200</td>\n","      <td>A1QE70QBJ8U6ZG</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I used this skirt for a Halloween costume and ...</td>\n","      <td>0000032034</td>\n","      <td>5.0</td>\n","      <td>1419292800</td>\n","      <td>A22CP6Z73MZTYU</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>This is thick enough that you can't see throug...</td>\n","      <td>0000032034</td>\n","      <td>4.0</td>\n","      <td>1418601600</td>\n","      <td>A22L28G8NRNLLN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2688126</th>\n","      <td>Love the shorts.. amazing comfortable, perfect...</td>\n","      <td>B01HJGAJ9O</td>\n","      <td>5.0</td>\n","      <td>1522886400</td>\n","      <td>A3TN0U64HONOPB</td>\n","    </tr>\n","    <tr>\n","      <th>2688127</th>\n","      <td>Superb quality</td>\n","      <td>B01HJGAJ9O</td>\n","      <td>5.0</td>\n","      <td>1519862400</td>\n","      <td>AJDQLM8PT3YWT</td>\n","    </tr>\n","    <tr>\n","      <th>2688128</th>\n","      <td>Works every time</td>\n","      <td>B01HJHHBHG</td>\n","      <td>5.0</td>\n","      <td>1521244800</td>\n","      <td>A3QK5ZLRE2KHLL</td>\n","    </tr>\n","    <tr>\n","      <th>2688129</th>\n","      <td>I have a briley bolt release paddle installed ...</td>\n","      <td>B01HJHHBHG</td>\n","      <td>5.0</td>\n","      <td>1509148800</td>\n","      <td>A3VDML80KNR9QQ</td>\n","    </tr>\n","    <tr>\n","      <th>2688130</th>\n","      <td>Works great</td>\n","      <td>B01HJHHBHG</td>\n","      <td>5.0</td>\n","      <td>1497139200</td>\n","      <td>A3ONWSRNZFNC3U</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2688131 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                reviewText        asin  \\\n","0                  What a spectacular tutu! Very slimming.  0000032034   \n","1        What the heck? Is this a tutu for nuns? I know...  0000032034   \n","2                        Exactly what we were looking for!  0000032034   \n","3        I used this skirt for a Halloween costume and ...  0000032034   \n","4        This is thick enough that you can't see throug...  0000032034   \n","...                                                    ...         ...   \n","2688126  Love the shorts.. amazing comfortable, perfect...  B01HJGAJ9O   \n","2688127                                     Superb quality  B01HJGAJ9O   \n","2688128                                   Works every time  B01HJHHBHG   \n","2688129  I have a briley bolt release paddle installed ...  B01HJHHBHG   \n","2688130                                        Works great  B01HJHHBHG   \n","\n","         overall  unixReviewTime      reviewerID  \n","0            5.0      1433289600  A180LQZBUWVOLF  \n","1            1.0      1427846400   ATMFGKU5SVEYY  \n","2            5.0      1421107200  A1QE70QBJ8U6ZG  \n","3            5.0      1419292800  A22CP6Z73MZTYU  \n","4            4.0      1418601600  A22L28G8NRNLLN  \n","...          ...             ...             ...  \n","2688126      5.0      1522886400  A3TN0U64HONOPB  \n","2688127      5.0      1519862400   AJDQLM8PT3YWT  \n","2688128      5.0      1521244800  A3QK5ZLRE2KHLL  \n","2688129      5.0      1509148800  A3VDML80KNR9QQ  \n","2688130      5.0      1497139200  A3ONWSRNZFNC3U  \n","\n","[2688131 rows x 5 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Droppen der Nan - Values der reviewTexte\n","review_df= review_df.dropna(subset=['reviewText'])\n","review_df.reset_index(inplace=True, drop=False)\n","review_df= review_df.drop(\"index\", axis=1)\n","\n","# Entfernung von Dublikaten\n","review_df.drop_duplicates(inplace=True)\n","review_df.reset_index(inplace=True, drop=False)\n","review_df= review_df.drop(\"index\", axis=1)\n","\n","# Ausgabe\n","review_df"]},{"cell_type":"markdown","metadata":{},"source":["### 4.3. Kurze Reviews löschen (<5 Zeichen)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Funktion zur bestimmung der Länge der Texte pro \"Dokument\" / Zeile\n","def count_text_len(in_string):\n","    text_len = len(in_string.split())\n","    return text_len"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Status ermittelten Sprache pro Dokument:   0%|          | 0/2688640 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["Status ermittelten Sprache pro Dokument: 100%|██████████| 2688640/2688640 [00:17<00:00, 154152.97it/s]\n"]}],"source":["# Anwendung der Funktion zur Ermittlung der Länge (Wörter) der Reviews\n","review_df[\"text_len\"] = Parallel(n_jobs=-1)(delayed(count_text_len)(in_string) for in_string in tqdm(review_df[\"reviewText\"], \n","                                                                                                desc =\"Status ermittelten Sprache pro Dokument: \",\n","                                                                                                total = len(review_df[\"reviewText\"])))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["397516\n"]}],"source":["# Löschen der zu kurzen Reviews\n","review_df = review_df[(review_df[\"text_len\"] >= 5)]"]},{"cell_type":"markdown","metadata":{},"source":["### 4.4. Selectieren der Jahre 2012 bis 2018"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Umrechnung der unixReviewTime in einen Timestamp\n","# Quelle: https://www.adamsmith.haus/python/answers/how-to-convert-a-column-of-unix-times-in-a-pandas-dataframe-to-datetimes-in-python\n","review_df[\"unixReviewTime\"] = pd.to_datetime(review_df[\"unixReviewTime\"], unit='s')\n","\n","# Aufsplitten in Jahre und Monate\n","review_df['year'] = pd.DatetimeIndex(review_df[\"unixReviewTime\"]).year"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### 4.5. Entfernen der nicht englischen Reviews"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Funktion zur Ermittlung der Sprache\n","# Quelle: https://pypi.org/project/LanguageIdentifier/\n","def identifier(in_string):\n","    language = predict(in_string)\n","    return language"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[]},{"name":"stderr","output_type":"stream","text":[]}],"source":["# Anwenden der Funktion auf den Datensatz\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","review_df[\"language\"] = Parallel(n_jobs=-1)(delayed(identifier)(in_string) for in_string in tqdm(review_df[\"reviewText\"], \n","                                                                                                desc =\"Status ermittelten Sprache pro Dokument: \",\n","                                                                                                total = len(review_df[\"reviewText\"])))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Entfernen der nicht englischen Reviews\n","review_df = review_df[(review_df[\"language\"] != \"en\")]"]},{"cell_type":"markdown","metadata":{},"source":["### 4.6. ..."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Erstellen eines neuen DataFrames mit den relevanten Spalten\n","review_df = review_df[[\"asin\"]]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Xfjzu4tP8A_"},"source":["## 5. Bereinigung der Textdaten\n","\n","Die Topic Modelling Modelle können lediglich numerische Daten verarbeiten. Aus diesem Grund wird im Rahmen der Textvorverarbeitung die Textdaten in ein berechenbares Zahlenformat konvertiert.\n","Hierfür existieren eine Reihe von Techniken zur Datenbereinigung und Vorverarbeitung:\n","\n","- Lowercasing\n","- Lemmatisierung\n","- Stemming\n","- Entfernung von Satzzeichen\n","- Entfernung von Stopwords\n","- Entfernung von Zahlen\n","- Entfernung von nicht ASCII konformen Wörtern\n","- Entfernung von http und Weblinks\n","\n","(Pomer, 2022)\n","\n","Um einen Konsistenten Datensatz für die weitere Verarbeitung und die Anwendung der Topic Modelling Modelle zu erhalten, werden die oben aufgeführten Bereinigungen durchgeführt.\n","\n","Doch befor die Textbereinigung stattfindet, werden alle Zeilen, welche in den reviewText keine Werte enthalten gelöscht und alle Zeilen rausgenommen, welche keine englischen Reviews enthalten. \n","Darüber hinaus werden die Spalten des Data Frame reduziert, indem nur die Spalten \"reviewText\" und \"year\" in einen neuen Data Frame aufgenommen werden. Dies hat den Grund, dass lediglich diese Spalten für die Modellierung benötigt werden und die Performance durch die verringerung der Daten verbessert werden kann."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8xAh772P8A_"},"outputs":[],"source":["# Neuer Data Frame, welcher die relevanten Spalten in englischer Sprache beinhaltet\n","text_df = test_df\n","text_df[\"reviewText\"] = text_df[\"reviewText\"].astype(str)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"757CHmG6P8A_"},"source":["### 5.1. Lowercasing\n","\n","Mithilfe des Lowercasings werden alle Großbuchstaben in Kleinbuchstaben konvertiert, da diese für das Verständnis der Texte nicht benötigt werden. Hierfür wird zuerst eine Funktion definiert, welche einen string in Kleinbuchstaben konvertiert (Pomer, 2022).\n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert.\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis des Lowercasing lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6J13qNqLP8BA"},"outputs":[],"source":["# Definition der Funktion, um alle Buchstaben in Kleinschreibung zu konvertieren\n","# Quelle: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","def to_lower(in_string):\n","    out_string = in_string.lower()\n","    return out_string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypL6Qq1rP8BA"},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_lower = np.vectorize(to_lower)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"reviewText\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_lower\"] = Parallel(n_jobs=-1)(delayed(vfunc_lower)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_lower'] = text_df['text_lower'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"reviewText\", \"text_lower\"]]], axis=1, sort = False, keys = [\"Lowercasing\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Mia6UXR7P8BA"},"source":["### 5.2. Lemmatisierung\n","Bei der Lemmatisierung handelt es sich um die Umwandlung von Wörtern in ihre Grundform unter Verwendung von Wörterbüchern (Pomer, 2022). Hierbei werden Pluralformen in Singularformen und unterschiedliche Zeitformen in das Präsenz umgewandelt (Yuzhe-Lu, 2018). Für die Lemmatisierung wird eine Funktion definiert, welche die Sätze in einzelen Token aufsplittet, desse POS-Tags ermittelt und basierend dieser Tags die Lemmatisierung mithilfe der Funktion \"lemmatize()\" durchführt (Johnson, 2023).  \n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert.\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis der Lemmatisierung lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTGKZE7sP8BA"},"outputs":[],"source":["# Definition der Funktion zur Lemmatisierung, um Wörter in ihre Grundform zu konvertieren\n","# Quelle 1: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","# Quelle 2: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#stanfordcorenlplemmatization\n","\n","def lemmatize(in_string):\n","    # Definition der notwendigen Parameter\n","    list_pos = 0                                  # Zuordnung einer Positionsnummer\n","    cleaned_str = ''                              # Leerer String für bereinigte Wörter\n","    text_token = nltk.word_tokenize(in_string)    # Tokenisieren der Sätze in einzelne Strings\n","    tagged_words = pos_tag(text_token)            # Grammatikalisches Tagging\n","    wnl = WordNetLemmatizer()                     # Klasse von NLTK für Lemmatisierung\n","    \n","    # Durchfürhung der Lemmatisation und Zusammenführung der Ergebnisse in einen String\n","    for word in tagged_words:\n","        if 'v' in word[1].lower():\n","            lemma = wnl.lemmatize(word[0], pos='v')\n","        else:\n","            lemma = wnl.lemmatize(word[0], pos='n')\n","        if list_pos == 0:\n","            cleaned_str = lemma\n","        else:\n","            cleaned_str = cleaned_str + ' ' + lemma\n","        list_pos += 1\n","    return cleaned_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2joK61R9P8BA"},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_lem = np.vectorize(lemmatize)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"text_lower\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_lemmatize\"] = Parallel(n_jobs=-1)(delayed(vfunc_lem)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_lemmatize'] = text_df['text_lemmatize'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_lower\", \"text_lemmatize\"]]], axis=1, sort = False, keys = [\"Lemmatizierung\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2Y21owjkP8BB"},"source":["### 5.3. Stemming\n","\n","Da die Lemmatisierung nicht alle Worte bereinigt hat, wie es zum Beispiel bei dem Wort \"quickly\" zu erkennen ist, welches eigentlich in \"quick\" konvertiert werden sollte, wird im folgenden zusätzlich der sogenannte Snowball-Stemming-Algorithmus implementiert.\n","\n","Im Gegensatz zur Lemmatisierung, beruft sich das Stemming nicht auf Wörterbücher, sondern entfernt die Sufixe und Präfixe der Wörter. Es findet daher kein Bezug auf den Kontext der Wörter statt\n","(Luber & Litzel, Was ist Stemming?, 2020).\n","\n","Zur Durchführung des Stemming wird eine Funktion definiert, welche die Sätze zuerst in einzelne Token unterteilt und den Snowball Stemmer auf diese mithilfe einer For-Schleife anwendet und wieder zu einem Satz verbindet (NLTK Project, 2023).\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert. Darüber hinaus werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis des Stemmings lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPOJlClUP8BB"},"outputs":[],"source":["# Definition der Funktion zur Durchführung des Stemmings\n","# Quelle: https://www.nltk.org/howto/stem.html\n","def stemming(in_string):\n","    # Definition der notwendigen Parameter\n","    text_token = nltk.word_tokenize(in_string)      # Tokenisieren der Sätze in einzelne Strings\n","    stemmer = SnowballStemmer(\"english\")            # Laden der Klasse zum Stemming\n","    \n","    # Durchfürhung des Stemmings und Zusammenführung der Ergebnisse in einen String\n","    cleaned_str = ' '.join([stemmer.stem(word) for word in text_token])\n","    return cleaned_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKTlsGRKP8BB"},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_stem = np.vectorize(stemming)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"text_lemmatize\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_stemming\"] = Parallel(n_jobs=-1)(delayed(vfunc_stem)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_stemming'] = text_df['text_stemming'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_lemmatize\", \"text_stemming\"]]], axis=1, sort = False, keys = [\"Stemming\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7riSNMCAP8BB"},"source":["### 5.4. Entfernung von Satzzeichen\n","\n","Nachdem die bereinigung der Wörter selbst abgeschlossen ist, werden nun die Satzzeichen aus dem Datensatz entfernt. Viele Satzzeichen verändern bei ihrem Fehlen den Kontext nicht groß, sodass diese problemlos bereinigt werden können. Jedoch ist zu beachten, dass beispielsweise Kommata oder Fragezeichen den Inhalt eines Textes verändern können, sodass es bei der Analyse zu Fehlinterpretationen kommt. Besonders zu beachten sind in diesem Zusammenhang Zahlen, da es bei dem Jahr \"1998\" und einem Geldwert \"19,98\" einen großen Unterschied macht, ob das Komma im Datensatz erhalten bleibt oder ob es verschwindet. Bei letzteren Fall hätten beide Zahlen den identischen Wert und das Ergebnis der Auswertung hätte keine Aussagekraft (Pomer, 2022). Aus diesem Grund soll im Folgenden mit diesem Wissen im Hinterkopf eine Bereinigung der Satzzeichen stattfinden. Treten bei dem Ergebnis der Analyse vermehrt Zahlen oder andere Unstimmigkeiten auf, müssen die Satzzeichen ggf. in die Modelle miteinbezogen werden.\n","\n","Für die Bereinigung werden folgende Zeichen durch einen leeren Wert mithilfe der PySpark functions \"col()\" und translate()\" ersetzt. Der Import der functions ist in Abschnitt 1. Installationen und Imports zu finden (Apache Spark, kein Datum):\n","\n","> '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Funktion zur Entfernung der Punktierung\n","# Quelle: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n","def punctation(in_string):\n","    cleaned_string = in_string.translate(str.maketrans(\"\",\"\",string.punctuation))\n","    return cleaned_string"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_punct = np.vectorize(punctation)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"text_stemming\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_punctation\"] = Parallel(n_jobs=-1)(delayed(vfunc_punct)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_punctation'] = text_df['text_punctation'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_stemming\", \"text_punctation\"]]], axis=1, sort = False, keys = [\"Punctation\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_fFtvo4kP8BC"},"source":["### 5.5. Entfernung von Stopwords\n","\n","Stoppwörter können in Vorbereitung auf das Topic Modelling ebenfalls herausgefiltert werden, da diese keinen Einfluss auf die Bedeutung des Kontextes haben. Aufgrund ihrer Häufigkeit in einem Text wird darüber hinaus der Datensatz drastisch reduziert, wordurch die Laufzeit sowie die Genauigkeit der Analyse verbessert werden kann. Die Auswahl der Stopword muss jedoch bedacht getroffen werden, um die Ergebnisse nicht zu verfälschen. So ist beispielsweise die Negation \"nicht\" in vielen Sprachen als Stopword eingestuft, kann jedoch den Kontext eines Satzes bei dessen Entfernung dramatisch verändern (Teja, 2020). Für jede Sprache existieren unterschiedliche Stoppwörter. Um diese aus den Daten herauszufiltern werden sogenannte Stoppwordlisten herangezogen (Pomer, 2022).\n","\n","Für die Bereinigung der Daten im Rahmen dieser Masterarbeit wird eine Liste der englischen Stopword der NLTK Library verwendet. \n","Zur Durchführung der Bereinigung wird eine Funktion definiert, welche die Token des Amazon Datensatzes mit der Liste der Stopwords vergleicht und einen bereinigten String kreiert, welcher keine Stopwords mehr enthält (Yuzhe-Lu, 2018)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSXjSm9mP8BC"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der Stopwords\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","def remove_stops(in_string):\n","    # Definition der notwendigen Parameter\n","    list_pos = 0                                  # Zuordnung einer Positionsnummer\n","    cleaned_str = ''                              # Leerer String für bereinigte Wörter\n","    text_token = nltk.word_tokenize(in_string)    # Tokenisieren der Sätze in einzelne Strings\n","    stop_words = stopwords.words('english')       # Bestimmen der Stopwords (für Englisch)\n","    stop_words.append('would')                    # Hinzufügen individueller Wörter zur Liste der Stopwords\n","\n","    # Durchfürhung der Entfernung der Stopwords und Zusammenführung der Ergebnisse in einen String\n","    for word in text_token:\n","        if word not in stop_words:\n","            if list_pos == 0:\n","                cleaned_str = word\n","            else:\n","                cleaned_str = cleaned_str + ' ' + word\n","            list_pos += 1\n","    return cleaned_str"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_stop = np.vectorize(remove_stops)\n","\n","# Vektorisierung der Spalte\n","test_df[\"text_stopwords\"] = vfunc_stop(text_df[\"text_punctation\"].to_numpy().reshape(-1))\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_punctation\", \"text_stopwords\"]]], axis=1, sort = False, keys = [\"Stopwords\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uDdHEQ3SP8BC"},"source":["### 5.6. Entfernung von Zahlen\n","\n","Im Rahmen der Masterarbeit wurde beschlossen, die Zahlen aus dem Datensatz zu entfernen, da die Annahme getroffen wurde, dass diese nicht zur Einteilung in die richtigen Kategorien beitragen.\n","Mithilfe der Regular expressions werden die Nummern 0 bis 9 in einerm Parameter gespeichert. Dieser wird daraufhin mit dem Input Text abgeglichen und bei einer Übereinstimmung wird die Zahl mit \"nichts\" ausgetauscht (Python Software Foundation, 2023). \n","\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4M50rwDP8BC"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der Zahlen\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","# Quelle: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","def remove_numbers(in_string):\n","    # Kompilieren der Zahlen 0-9 in ein Ausdrucksmuster\n","    num_re = re.compile('(\\\\d+)')\n","\n","    # Ersetzen der Zahlen durch \"nichts\"\n","    cleaned_str = \" \".join((re.sub(num_re, \"\", in_string)).split()) \n","    return cleaned_str"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_numbers = np.vectorize(remove_numbers)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"text_stopwords\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_numbers\"] = Parallel(n_jobs=-1)(delayed(vfunc_numbers)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_numbers'] = text_df['text_numbers'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_stopwords\", \"text_numbers\"]]], axis=1, sort = False, keys = [\"Numbers\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PfreefaqP8BD"},"source":["### 5.7. Entfernung von nicht ASCII konvormen Wörtern\n","\n","ASCII ist die Kurzform für „American Standard Code for Information Interchange“ und dient zur standartisierten Darstellung von Zeichen in elektronischer Form (IONOS SE, 2022). Zu beachten ist, dass es bei diesem um einen amerikanischen Standart handelt und daher keine Sonderzeichen, wie „ß“ oder „é, á“ berücksichtigt und daher beispielsweise bei deutschen Datensätzen das ergebnis verfälschen könnte (Pomer, 2022). Da der Amazondatensatz zufor auf die Sprache geprüft und bereinigt wurde, beinhaltet dieser lediglich englische Sätze, wodurch der ASCII Standard verwendet werden kann.\n","\n","Zur Bereinigung wird die untenstehende Funktion definiert, welche den Unicode der wörter ermittelt und diese mit den Unicodes aus dem ASCII Standard abgleicht. Lediglich die Wörter, welche zwischen 0 und 128 liegen werden im Datensatz beibehalten.\n","Im Anschluss wird mithilfe von PySpark die Funktion auf den Data Frame angewendet und die bereinigten Ergebnisse in einer neuen Spalte gespeichert (Johnson, 2023).\n","Abschließend werden alle \"non-type-values\" aus dem Datensatz herausgefilter, bevor dieser den nächsten Bereinigungsschritt durchläuft (Yuzhe-Lu, 2018).\n","Das Ergebnis für die Entfernung der Satzzeichen lässt sich in der Ausgabe nachverfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0_cPu-UP8BD"},"outputs":[],"source":["# Definition der Funktion zur Entfernung der ASCII characters\n","# Quelle: https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb\n","def remove_non_ascii_words(in_string):\n","    ''' Returns the string without non ASCII characters'''\n","    cleaned_str = ''.join(word for word in in_string if 0 < ord(word) < 128)\n","    return cleaned_str"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Anwenden der Funktion auf den Datensatz\n","# Erstellung einer Vektor Funktion\n","vfunc_ascii = np.vectorize(remove_non_ascii_words)\n","\n","# Vektorisierung der Spalte\n","text_vec = text_df[\"text_numbers\"].to_numpy().reshape(-1)\n","\n","# Durchführen der Sprachanalyse mit Beschleunigung\n","text_df[\"text_ascII\"] = Parallel(n_jobs=-1)(delayed(vfunc_ascii)(in_string) for in_string in text_vec)\n","\n","# Konvertieren der numpy.ndarrays zu einem string\n","text_df['text_ascII'] = text_df['text_ascII'].apply(str)\n","\n","# Ausgabe\n","pd.concat([text_df[[\"text_numbers\", \"text_ascII\"]]], axis=1, sort = False, keys = [\"ASCII Characters\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"U7gcRdOpP8BF"},"source":["### 5.9. Merge und select\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-k2PUjWMP8BF"},"outputs":[],"source":["# Merge der Meta Daten mit dem bereinigeten Datensatz\n","merged_df = text_df.merge(meta_df, on=\"asin\", how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLlh5UbeP8BF"},"outputs":[],"source":["# Neuen Data Frame erzeugen, welcher nur die relevanten Spalten enhält\n","cleaned_data_df = merged_df[[\"reviewText\", \"text_ascII\", \"year\", \"month\", \"overall\", \"brand\", \"price\", \"title\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleaned_data_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Export"]},{"cell_type":"markdown","metadata":{},"source":["Apache Spark. (17. 02 2023). Download Apache Spark™. Abgerufen am 20. 02 2023 von spark.apache.org: https://spark.apache.org/downloads.html\n","\n","Erick. (12. 02 2018). How to unzip gz file using Python. Abgerufen am 24. 10 2022 von stackoverflow.com: https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n","\n","Free Software Foundation, I. (02. 04 2022). GNU Gzip: General file (de)compression. Abgerufen am 20. 02 2023 von gnu.org: https://www.gnu.org/software/gzip/manual/gzip.html\n","\n","IONOS SE. (05. 04 2022). ASCII – Erklärung und Beispiele. Abgerufen am 08. 03 2023 von ionos.de: https://www.ionos.de/digitalguide/server/knowhow/ascii-american-standard-code-for-information-interchange/\n","\n","Johnson, D. (21. 01 2023). POS-Tagging mit NLTK und Chunking in NLP [BEISPIELE]. Abgerufen am 21. 02 2023 von guru99.com: https://www.guru99.com/pos-tagging-chunking-nltk.html#1\n","\n","Luber, D.-I. & Litzel, N. (30. 11 2020). Was ist Stemming? Abgerufen am 22. 02 2023 von bigdata-insider.de: https://www.bigdata-insider.de/was-ist-stemming-a-980852/#:~:text=Abgrenzung%20von%20Stemming%20und%20Lemmatisierung&text=Stemming%2DAlgorithmen%20arbeiten%20meist%20mit,)%2C%20um%20Stammformen%20zu%20finden.\n","\n","NLTK Project. (02. 01 2023). Natural Language Toolkit - Sample usage for stem. Abgerufen am 22. 02 2023 von nltk.org: https://www.nltk.org/howto/stem.html\n","\n","Oracle. (17. 01 2023). Download Java for Windows. Abgerufen am 20. 02 2023 von java.com: https://www.java.com/download/ie_manual.jsp\n","\n","Pomer, L. (03. 08 2022). Textklassifikation – Vorverarbeitung der Daten. Abgerufen am 14. 02 2023 von thecattlecrew.net: https://thecattlecrew.net/2022/08/03/textklassifikation-vorverarbeitung-der-daten/\n","\n","Preusler, S. (13. 03 2020). Pyspark und Jupyter Notebook Anleitung für Windows. Abgerufen am 14. 01 2023 von https://medium.com/@stefan.preusler/pyspark-und-jupyter-notebook-anleitung-f%C3%BCr-windows-7317f2c968c4\n","\n","Python Software Foundation. (2023). re — Regular expression operations¶. Abgerufen am 08. 03 2023 von docs.python.org: https://docs.python.org/3/library/re.html\n","\n","Shah, A., 2018. How to setup Apache Spark(PySpark) on Jupyter/IPython Notebook?. [Online] \n","Available at: https://medium.com/@ashish1512/how-to-setup-apache-spark-pyspark-on-jupyter-ipython-notebook-3330543ab307\n","[Accessed 14 01 2023].\n","\n","Sarkar, T. (12. 11 2018). How to set up PySpark for your Jupyter notebook. Abgerufen am 14. 01 2023 von opensource.com: https://opensource.com/article/18/11/pyspark-jupyter-notebook\n","\n","SparkBy{Examples}. (06. 12 2020). PySpark Liest JSON-Datei in DataFrame. Abgerufen am 14. 01 2023 von https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline\n","\n","Teja, S. (10. 06 2020). Stop Words in NLP. Abgerufen am 24. 02 2023 von medium.com: https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\n","\n","Wuttke, L. (2022). Einführung in Apache Spark: Komponenten, Vorteile und Anwendungsbereiche. Abgerufen am 05. 01 2023 von datasolut.com: https://datasolut.com/was-ist-spark/#was-ist-pyspark\n","\n","Yuzhe-Lu. (10. 12 2018). Topic-Modeling-on-Amazon-Reviews-using-LDA/2_LDA_Data_Processing.ipynb. Abgerufen am 14. 02 2023 von https://github.com/rrathgithub/Topic-Modeling-on-Amazon-Reviews-using-LDA/blob/master/2_LDA_Data_Processing.ipynb"]}],"metadata":{"colab":{"collapsed_sections":["V7GIDkK7P8A1","uSfhN2M6P8A7","-TFWdcL5P8A8","i3qdIqPUP8A8","jLdys3KNP8A9","g2mkmZnrP8A-","e-uyPtj6P8A-","OnSQCQVaP8A-","skfU9c9mP8A_","6yBWS5TyP8A_","oKZnPIpCoRFN","AQlHynY2oUWW","Wbdo9EOdoXt0","aG7sXbVjoaCU","757CHmG6P8A_","Mia6UXR7P8BA","2Y21owjkP8BB","7riSNMCAP8BB","_fFtvo4kP8BC","uDdHEQ3SP8BC","PfreefaqP8BD","zkD4MJzAP8BE","jVvw4YnaP8BE","U7gcRdOpP8BF","oHZ2e79BP8BF","z3u8WV7eP8BF","PKgkXH0wP8BF"],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4aa37d1a4c10a2e7b72a0d527e28b9c54c47ddea71f47cc08f2d095dc3868839"}}},"nbformat":4,"nbformat_minor":0}
